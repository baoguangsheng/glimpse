# <img src="doc/glimpse.png" alt="glimpse" width="64"/>Glimpse
**This code is for our ICLR 2025 paper "Glimpse: Enabling White-Box Methods to Use Proprietary Models for Zero-Shot LLM-Generated Text Detection"**, where we borrow some code from [Fast-DetectGPT](https://github.com/baoguangsheng/fast-detect-gpt).

[Paper](https://arxiv.org/abs/2412.11506)
| [LocalDemo](#local-demo)
| [OnlineDemo](https://aidetect.lab.westlake.edu.cn/)
| [OpenReview](https://openreview.net/forum?id=an3fugFA23)

* ðŸ”¥ Local and online demos are ready!

## Brief Intro
<table class="tg"  style="padding-left: 30px;">
  <tr>
    <th class="tg-0pky">Method</th>
    <th class="tg-0pky">ChatGPT</th>
    <th class="tg-0pky">GPT-4</th>
    <th class="tg-0pky">Claude-3<br/>Sonnet</th>
    <th class="tg-0pky">Claude-3<br/>Opus</th>
    <th class="tg-0pky">Gemini-1.5<br/>Pro</th>
    <th class="tg-0pky">Avg.</th>
  </tr>
  <tr>
    <td class="tg-0pky">Fast-DetectGPT<br/>(Open-Source: gpt-neo-2.7b)</td>
    <td class="tg-0pky">0.9487</td>
    <td class="tg-0pky">0.8999</td>
    <td class="tg-0pky">0.9260</td>
    <td class="tg-0pky">0.9468</td>
    <td class="tg-0pky">0.8072</td>
    <td class="tg-0pky">0.9057</td>
  </tr>
  <tr>
    <td class="tg-0pky">Glimpse (Fast-DetectGPT)<br/>(Proprietary: gpt-3.5)</td>
    <td class="tg-0pky"><b>0.9766</b><br/>(<b>â†‘54%</b>)</td>
    <td class="tg-0pky"><b>0.9411</b><br/>(<b>â†‘41%</b>)</td>
    <td class="tg-0pky"><b>0.9576</b><br/>(<b>â†‘43%</b>)</td>
    <td class="tg-0pky"><b>0.9689</b><br/>(<b>â†‘42%</b>)</td>
    <td class="tg-0pky"><b>0.9244</b><br/>(<b>â†‘61%</b>)</td>
    <td class="tg-0pky"><b>0.9537</b><br/>(<b>â†‘51%</b>)</td>
  </tr>
</table>

Glimpse achieves significant improvements in detection accuracy (AUROC) across latest source LLMs. The notion "â†‘" indicates the improvement relative to the remaining space, calculated by "(new - old) / (1.0 - old)".

## Local Demo
Run following command locally for an interactive demo:
```
python scripts/local_infer.py  --api_key <openai API key>  --scoring_model_name davinci-002 
```
An example looks like
```
Please enter your text: (Press Enter twice to start processing)
å·¥ä½œé‡å’Œå·¥ä½œå¼ºåº¦ä¼šæ ¹æ®é“¶è¡Œçš„ä¸åŒè€Œæœ‰æ‰€ä¸åŒã€‚ä½†ä¸€èˆ¬æ¥è¯´ï¼Œä½œä¸ºä¸šåŠ¡å‘˜éœ€è¦åœ¨å·¥ä½œä¸­éœ€è¦é¢å¯¹å„ç±»å®¢æˆ·ï¼Œä»¥åŠæ‰¿æ‹…ä¸€å®šçš„å·¥ä½œåŽ‹åŠ›å’Œä¸šç»©æŒ‡æ ‡ï¼Œå› æ­¤è¿™ä¸ªèŒä¸šç¡®å®žéœ€è¦ç›¸å½“çš„åŠªåŠ›å’Œä¸æ–­çš„è‡ªæˆ‘æé«˜ã€‚

Glimpse criterion is -0.3602, suggesting that the text has a probability of 69% to be machine-generated.
```

## Environment
* Python3.12
* Setup the environment:
  ```pip install -r requirements.txt```
  
(Notes: the baseline methods are run on 1 GPU of Tesla A100 with 80G memory, while Glimpse is run on a **CPU** environment.)

## Experiments
Following folders are created for our experiments:
* ./exp_main -> experiments with five latest LLMs as the source model (main.sh).
* ./exp_langs -> experiments on six languages (langs.sh).

(Notes: we share the data and results for convenient reproduction.)

### Citation
If you find this work useful, you can cite it with the following BibTex entry:

    @articles{bao2025glimpse,
      title={Glimpse: Enabling White-Box Methods to Use Proprietary Models for Zero-Shot LLM-Generated Text Detection},
      author={Bao, Guangsheng and Zhao, Yanbin and He, Juncai and Zhang, Yue},
      booktitle={The Thirteenth International Conference on Learning Representations},
      year={2025}
    }

