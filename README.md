# <img src="assets/glimpse.png" alt="glimpse" width="64"/>Glimpse
**This code is for our ICLR 2025 paper "Glimpse: Enabling White-Box Methods to Use Proprietary Models for Zero-Shot LLM-Generated Text Detection"**, where we borrow some code from [Fast-DetectGPT](https://github.com/baoguangsheng/fast-detect-gpt).

[Paper](https://arxiv.org/abs/2412.11506)
| [LocalDemo](#local-demo)
| [OnlineDemo](https://aidetect.lab.westlake.edu.cn/)
| [OpenReview](https://openreview.net/forum?id=an3fugFA23)

* 9/4/2025: The demo server is facing a hardware problem, and we are actively working on a solution.
* ðŸ”¥ [4/14/2025] Free [API access](https://aidetect.lab.westlake.edu.cn/#/apidoc) to the detectors is ready! 
* ðŸ”¥ [2/10/2025] Local and online demos are ready! As OpenAI and AzureOpenAI have discontinued the legacy Completion API for GPT-3.5-turbo and GPT-4, our demo is limited to davinci and babbage models.

## Brief Intro
Glimpse serves as a bridge between white-box methods, which rely on local LLMs for scoring, and proprietary LLMs. It estimates full distributions based on partial observations from API-based models. Our empirical analysis demonstrates that detection methods leveraging these estimated distributions **achieve detection accuracies comparable to those obtained using real distributions**.

The main results are as follows, where Fast-DetectGPT using the proprietary GPT-3.5 achieves significantly better detection accuracies than that using open-source GPT-Neo-2.7B, **especially on multiple languages**.
<img src="assets/main-results.png" alt="main results" width="640"/>


## Local Demo
Run following command locally for an interactive demo:
```
python scripts/local_infer.py  --api_key <openai API key>  --scoring_model_name davinci-002 
```
An example looks like
```
Please enter your text: (Press Enter twice to start processing)
å·¥ä½œé‡å’Œå·¥ä½œå¼ºåº¦ä¼šæ ¹æ®é“¶è¡Œçš„ä¸åŒè€Œæœ‰æ‰€ä¸åŒã€‚ä½†ä¸€èˆ¬æ¥è¯´ï¼Œä½œä¸ºä¸šåŠ¡å‘˜éœ€è¦åœ¨å·¥ä½œä¸­éœ€è¦é¢å¯¹å„ç±»å®¢æˆ·ï¼Œä»¥åŠæ‰¿æ‹…ä¸€å®šçš„å·¥ä½œåŽ‹åŠ›å’Œä¸šç»©æŒ‡æ ‡ï¼Œå› æ­¤è¿™ä¸ªèŒä¸šç¡®å®žéœ€è¦ç›¸å½“çš„åŠªåŠ›å’Œä¸æ–­çš„è‡ªæˆ‘æé«˜ã€‚

Glimpse criterion is -0.3602, suggesting that the text has a probability of 69% to be machine-generated.
```

## Environment
* Python3.12
* Setup the environment:
  ```pip install -r requirements.txt```
  
(Notes: the baseline methods are run on 1 GPU of Tesla A100 with 80G memory, while Glimpse is run on a **CPU** environment.)

## Experiments
Following folders are created for our experiments:
* ./exp_main -> experiments with five latest LLMs as the source model (main.sh).
* ./exp_langs -> experiments on six languages (langs.sh).

(Notes: we share the data and results for convenient reproduction.)

### Citation
If you find this work useful, you can cite it with the following BibTex entry:

    @articles{bao2025glimpse,
      title={Glimpse: Enabling White-Box Methods to Use Proprietary Models for Zero-Shot LLM-Generated Text Detection},
      author={Bao, Guangsheng and Zhao, Yanbin and He, Juncai and Zhang, Yue},
      booktitle={The Thirteenth International Conference on Learning Representations},
      year={2025}
    }

